{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks can be used to classify text data just like they can be used to classify image data. The process for classifying text data is very similar to the process for classifying images. First, the data must be converted into a form that the network can learn, a training and testing set must be created, and then the classifier must be trained on the data. \n",
    "\n",
    "This notebook examines multiple different ways to create a text classifier, first demonstrating how to use word embeddings on a small dataset, and then adapting the process to larger dataset. A LSTM network is also implemented for the sake of comparison. We'll be using Keras to implement these networks.\n",
    "\n",
    "To begin with, we're going to need to load in the data and convert the words in our text data to representations that our deep learning model can work with. These representations are referred to as \"word embeddings\" and they are numerical representations contained with a geometric space called \"embedding space\". Similar words will have similar representations in embedding space, and because of this the network is capable of learning patterns and even reasoning by analogy.\n",
    "\n",
    "We'll start out by choosing a small dataset to work on, which in this case will be the Sentiment Labelled Sentences Data Set, found at the UCI Machine Learning Repository. It contains many reviews from Yelp, IMDB, and Amazon and classifies these reviews as positive or negative. We'll need to tokenize these reviews and then we can form embeddings of them and pass them in to our deep neural network. \n",
    "\n",
    "Let's start out by importing all the libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dense, GlobalMaxPooling1D, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the .txt format, but it will be easier to work with if we load it into a CSV. We'll use Pandas to create the dataframe. Thankfully, we can distingusih between the text/review and the label of that review by dividing where there is a tab in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text to CSV\n",
    "def csv_convert(input_data, data_source):\n",
    "    # separate labels and sentences on tabs\n",
    "    df = pd.read_csv(input_data, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = data_source\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to set the directory for the individual text files and convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_data = \"/sentiment labelled sentences/amazon_cells_labelled.txt\"\n",
    "imdb_data = \"/sentiment labelled sentences/imdb_labelled.txt\"\n",
    "yelp_data = \"/sentiment labelled sentences/yelp_labelled.txt\"\n",
    "\n",
    "amz_data = csv_convert(amz_data, \"Amazon\")\n",
    "imdb_data = csv_convert(imdb_data, \"IMDB\")\n",
    "yelp_data = csv_convert(yelp_data, \"Yelp\")\n",
    "\n",
    "print(amz_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the three individual datasets into one large dataset. Then we can pull out the training features, which will be the sentences, and the labels from the completed data CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([amz_data, imdb_data, yelp_data])\n",
    "df_complete.to_csv(\"sentiment_labelled_complete.csv\")\n",
    "print(amz_data.head(10))\n",
    "\n",
    "# Separate out the features and labels from the CSV\n",
    "\n",
    "features = df_complete['sentence'].values\n",
    "labels = df_complete['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the extremely useful `train_test_split` function from Scikit-learn to divide our features and labels up into training/testing features and training/testing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training/testing features and labels\n",
    "# Still need to tokenize them\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data split into training and testing sets, we need to tokenize the data. This makes numerical representations of the text data that our network can intepret. There are a variety of options for tokenizing data, but the simplest way to do this is probably to use the built in `Tokenizer` that comes with Keras. First, we have to declare an instance of the tokenizer and tell it the total number of words we want to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer, fit it on the text data\n",
    "# use text_to_sequences to actually conver the features to tokens\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Need this for the creation of the network\n",
    "vocab_len = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every sequence will be the same length. This represents a bit of a problem for the classifier, but we can get around this by \"padding\" the data. Padding will insert zeroes where there isn't data to make sure all the data is the same length. We'll also set the max length to 100 characters so that no sequence can go past that length. By doing both these things we make sure all the sequences are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Selects size/complexity of the embeddings\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up oup Convolutional Neural Network, embed the data, then train on the data. When creating the embeddings, we set a dimension for the embedding. The embedding dimension defines how complex the representation of the sequence is, with more complex representations being more useful to the classifier when it attempts to learn patterns.\n",
    "\n",
    "After this, we insert a convolutional layer into the network. This convolutional layer is what actually analyzes our embedding data to find relevant patterns, detecting features likely to be important to the meaning of the sentence and hence the class/label. We'll then downsample this data, which is extremely complex, forming simpler snapshots of the data. The Max Pooling layer abstracts away any data deemed unimportant by simply taking the maximum value of regions of the convolutions. By taking the biggest/maximum value of a portion of the convolutions, the most important information in that region is maintained while the representation becomes simpler.\n",
    "\n",
    "Finally, we pass the pooled data into the dense, fully-connected layers in the model. These layers are what learn the patterns found by the convolutional layers. For a problem this small, this number of dense layers may actually be overkill and it is possible that the model could be overfitting, but for this simple demonstration we can experiment with the results anyway. Finally, we just compile the model - with our chosen optimizer, loss, and metrics - and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the text classification model\n",
    "def sentiment_model(embedding_dim):\n",
    "    model = Sequential()\n",
    "    # include the word embedding layers\n",
    "    model.add(Embedding(vocab_len, embedding_dim, input_length=maxlen))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = sentiment_model(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now fit the model to train it and save it as a variable, so that we can access metrics like accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test), batch_size=10)\n",
    "\n",
    "_, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make a function that can be used to visualize accuracy and loss for the training and validation sets. We can keep using this function for our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'r', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Accuracy Over EPochs')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'r', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rising validation loss could be indicative of overfitting. It seems likely that the dataset we are experimenting with is just too small to for the complexity of our Convolutional Network. Now that we have a good idea of how to transform data and create an embedding network, let's create a classifier for a more complex problem. Keras has an IMDB review dataset built into it, intended for use with text classifiers. It contains reviews labeled as positive or negative, much like our other dataset.\n",
    "\n",
    "Loading in this dataset is a bit convoluted, as the current version of Keras contains a bug which prevents it from being use properly. We can get around this by temporarily modifying the parameters of the dataset which are causing the issue. The lambda function you see below allows us to work around the bug. We can then easily load the data into training and testing sets, and if we are interested we can get an idea of how many labels and features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass bug in current version of Keras, allows use of imdb dataset\n",
    "# modifies the default parameters of numpy's load function\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=10000)\n",
    "\n",
    "# get features and labels, make sure they are the same number\n",
    "features = np.concatenate((X_train, X_test), axis=0)\n",
    "labels = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pad the sequences of the data again. This time we'll use 500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our new convlutional model. Like before, we set the input data to a chosen number of input sequences and set our output number when we create the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(max_words):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000, 64, input_length=max_words))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = conv_model(max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now like before we can just fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy[1]*100))\n",
    "plot_history(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one last thing. We're going to try implementing a specific type of CNN called a Long Short-Term Memory network, or LSTM. LSTMs excel at handing chronological data, where the order of the data matters.  LSTMs are able to \"remember\" data from earlier time steps in the training and take it into account. \n",
    "\n",
    "Word order matters when trying to make sense of the meaning of a sentence, so an LSTM could potentially be useful in this case. The LSTM may not prove extremely useful as presence of positive and negative words may have an outsized effect on the classification of the features, moreso than the order of the words in our training examples. Nontheless, LSTMs are powerful tools and it's a good idea to be aware of their use cases.\n",
    "\n",
    "We just define the LSTM model the same way that we define the other models, including LSTM layers instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(max_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000, 32, input_length=max_words))\n",
    "    model.add(LSTM(64, dropout=0.2, return_sequences=True))\n",
    "    model.add(LSTM(128, dropout=0.2))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = LSTM_model(max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just have to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "# \"format\" this\n",
    "accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy[1]*100))\n",
    "\n",
    "plot_history(records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
